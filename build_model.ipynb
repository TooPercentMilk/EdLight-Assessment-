{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "\n",
    "import gensim.downloader\n",
    "embed = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "base_directory = path + '/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1(path, img_name):\n",
    "    # load the image from file\n",
    "    image = load_img(path, target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "\n",
    "    # plt.imshow(image[0, ..., 0])\n",
    "    # plt.axis('off')  # Turn off the axis\n",
    "    # plt.title(image_id)\n",
    "    # plt.show()  \n",
    "    # store feature\n",
    "    return image_id, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method2(path, img_name):\n",
    "    target_height = 224\n",
    "    target_width = 224\n",
    "    # Load and preprocess image\n",
    "    image = tf.io.read_file(path) \n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    # Calculate the aspect ratio of the original image\n",
    "    original_height, original_width, _ = tf.unstack(tf.shape(image))\n",
    "    aspect_ratio = tf.cast(original_width, tf.float32) / tf.cast(original_height, tf.float32)\n",
    "\n",
    "    # Calculate the new dimensions while preserving the aspect ratio\n",
    "    if aspect_ratio > 1.0:\n",
    "        new_width = target_width\n",
    "        new_height = tf.cast(target_width / aspect_ratio, tf.int32)\n",
    "    else:\n",
    "        new_height = target_height\n",
    "        new_width = tf.cast(target_height * aspect_ratio, tf.int32)\n",
    "\n",
    "    # Resize and pad the image to the target size\n",
    "    resized_image = tf.image.resize_with_pad(image, target_height, target_width, method='bilinear')\n",
    "    resized_image = tf.image.convert_image_dtype(resized_image, tf.uint8)\n",
    "    resized_image = tf.expand_dims(resized_image, 0)\n",
    "\n",
    "    image_id = img_name.split('.')[0]\n",
    "\n",
    "    # plt.imshow(resized_image[0, ..., 0])\n",
    "    # plt.axis('off')\n",
    "    # plt.title(image_id)\n",
    "    # plt.show()  \n",
    "\n",
    "    feature = model.predict(resized_image, verbose=0)\n",
    "    return image_id, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "img_features = {}\n",
    "working_directory = base_directory + '/images'\n",
    "\n",
    "target_height = 224\n",
    "target_width = 224\n",
    "count = 0\n",
    "\n",
    "for img_name in os.listdir(working_directory):\n",
    "    if img_name.endswith('.jpg'):\n",
    "        path = working_directory + '/' + img_name\n",
    "\n",
    "        imid, fe = method2(path, img_name)\n",
    "        # imid, fe = method2(path, img_name)\n",
    "        img_features[imid] = fe\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the features because processing the data takes a while without a GPU\n",
    "pickle.dump(img_features, open(os.path.join(base_directory, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = base_directory + '/descriptions.csv'\n",
    "captions = pd.read_csv(filename)\n",
    "captions['file'] = captions['file'].str[:-4]\n",
    "captions = captions[captions['file'] != 'a6a35734-ee74-42bd-a13a-dfa2b683fcda'] # outlier in length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student filled in two given sets of double number lines. In the first double number line, the student completed the top number line with \"12\\frac{1}{2}, 25, 37\\frac{1}{2} , 50, 62\\frac{1}{2} , 75, 87\\frac{1}{2} , 100, 112\\frac{1}{2} \". Note that 100 was pre-filled in the diagram. The student completed the bottom line with \"2, 3, 4, 5, 6, 7, 8, 9\". In the second double number line, the student completed the top number line with \"40, 60, 80\". The student completed the bottom number line with \"1, 2, 3\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process lines\n",
    "# captions['tokens'] = captions['description'].apply(lambda description: [word.lower() for word in word_tokenize(description)])\n",
    "\n",
    "# Create a dictionary to map labels to tokenized captions\n",
    "mappings = {}\n",
    "\n",
    "for index, row in captions.iterrows():\n",
    "    label = row['file']\n",
    "    tokens = row['description']\n",
    "    mappings[label] = tokens\n",
    "print(mappings['2d33d6a3-d2eb-496d-9ac6-832911e178f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bos&gt; one triangle that is identical to the or...</td>\n",
       "      <td>48d27a28-851f-4fcb-82b1-b252ea5d8295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;bos&gt; one triangle drawn above the original tr...</td>\n",
       "      <td>420ec849-d0da-4f45-aed0-645bfa3b1d62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bos&gt; x written next to the original triangle ...</td>\n",
       "      <td>c5cc8cbc-7844-405b-a204-6aca67ef4384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bos&gt; student drew and shaded an identical tri...</td>\n",
       "      <td>abc6bf50-9b06-4c09-9e7c-90a1403ff860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bos&gt;  a shaded triangle drawn above the origi...</td>\n",
       "      <td>231b00f3-c151-48a0-a19c-d17882ba7baf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;bos&gt; the student filled in  in the top number...</td>\n",
       "      <td>1e49326b-9fe8-4c5a-b7e8-fa1eea1e9a0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;bos&gt; the student labeled the bottom number li...</td>\n",
       "      <td>332ea863-c4fa-4905-96bb-32fc71aa5ffe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;bos&gt; the student filled in two given sets of ...</td>\n",
       "      <td>8d32fce4-90ee-4678-910f-5fb0f60d4dce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;bos&gt; the student filled in two given sets of ...</td>\n",
       "      <td>9ca12d51-d5e0-41ee-9a9b-fd74a95f8982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;bos&gt; the student filled in two given sets of ...</td>\n",
       "      <td>2d33d6a3-d2eb-496d-9ac6-832911e178f1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  <bos> one triangle that is identical to the or...   \n",
       "1  <bos> one triangle drawn above the original tr...   \n",
       "2  <bos> x written next to the original triangle ...   \n",
       "3  <bos> student drew and shaded an identical tri...   \n",
       "4  <bos>  a shaded triangle drawn above the origi...   \n",
       "5  <bos> the student filled in  in the top number...   \n",
       "6  <bos> the student labeled the bottom number li...   \n",
       "7  <bos> the student filled in two given sets of ...   \n",
       "8  <bos> the student filled in two given sets of ...   \n",
       "9  <bos> the student filled in two given sets of ...   \n",
       "\n",
       "                                   file  \n",
       "0  48d27a28-851f-4fcb-82b1-b252ea5d8295  \n",
       "1  420ec849-d0da-4f45-aed0-645bfa3b1d62  \n",
       "2  c5cc8cbc-7844-405b-a204-6aca67ef4384  \n",
       "3  abc6bf50-9b06-4c09-9e7c-90a1403ff860  \n",
       "4  231b00f3-c151-48a0-a19c-d17882ba7baf  \n",
       "5  1e49326b-9fe8-4c5a-b7e8-fa1eea1e9a0c  \n",
       "6  332ea863-c4fa-4905-96bb-32fc71aa5ffe  \n",
       "7  8d32fce4-90ee-4678-910f-5fb0f60d4dce  \n",
       "8  9ca12d51-d5e0-41ee-9a9b-fd74a95f8982  \n",
       "9  2d33d6a3-d2eb-496d-9ac6-832911e178f1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_caption(caption):\n",
    "    # Convert to lowercase\n",
    "    caption = caption.lower()\n",
    "    # Remove special characters and punctuation using regular expressions\n",
    "    caption = re.sub(r'[^a-z\\s]', '', caption)\n",
    "    caption = '<bos> ' + caption + ' <eos>'\n",
    "    return caption\n",
    "\n",
    "captions['description'] = captions['description'].apply(clean_caption)\n",
    "captions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(captions['description'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1782\n",
      "Max lenght:  180\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size: \", vocab_size)\n",
    "max_length = max(len(caption.split()) for caption in captions['description'])\n",
    "print(\"Max lenght: \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train-test split\n",
    "image_ids = list(mappings.keys())\n",
    "split = int(len(image_ids) * 0.8)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            caption = mapping[key]\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "            # split the sequence into X, y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pairs\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                \n",
    "                # store the sequences\n",
    "                X1.append(features[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1782, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embed.vectors.shape[1]))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embed:\n",
    "        embedding_matrix[i] = embed[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 180)]        0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 180, 100)     178200      ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096)         0           ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 180, 100)     0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 256)          1048832     ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 256)          365568      ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 256)          0           ['dense_20[0][0]',               \n",
      "                                                                  'lstm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 256)          65792       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 1782)         457974      ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,116,366\n",
      "Trainable params: 1,938,166\n",
      "Non-trainable params: 178,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:25:23.796499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-10-27 19:25:23.797639: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-10-27 19:25:23.799123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "# Use GloVe embeddings for the embedding layer\n",
    "se1 = Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_length, trainable=False)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    steps = len(train) // batch_size\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # create data generator\n",
    "        generator = data_generator(train, mappings, img_features, tokenizer, batch_size)\n",
    "        # fit for one epoch\n",
    "        model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "        print(\"Epoch 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:34:33.511034: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 341s 3s/step - loss: 4.5817\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:40:14.597683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 355s 3s/step - loss: 4.1900\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:46:09.743047: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 360s 3s/step - loss: 3.8615\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:52:09.552994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 352s 3s/step - loss: 3.5657\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:58:02.159261: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 347s 3s/step - loss: 3.2951\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 20:03:49.195186: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 349s 3s/step - loss: 3.0690\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 20:09:38.604624: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 386s 3s/step - loss: 2.9265\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 20:16:04.954680: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 358s 3s/step - loss: 2.7534\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 20:22:03.698514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 361s 3s/step - loss: 2.6235\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 20:28:04.882257: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 370s 3s/step - loss: 2.4968\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(base_directory + '/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to generate captions\n",
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = '<bos>'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "      \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brad/Desktop/EdLight Assessment/build_model.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m captions \u001b[39m=\u001b[39m mappings[key]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# predict the caption for image\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred \u001b[39m=\u001b[39m predict_caption(model, img_features[key], tokenizer, max_length) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# split into words\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m actual_captions \u001b[39m=\u001b[39m [caption\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m caption \u001b[39min\u001b[39;00m captions]\n",
      "\u001b[1;32m/Users/brad/Desktop/EdLight Assessment/build_model.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sequence \u001b[39m=\u001b[39m pad_sequences([sequence], max_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# predict next word\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m yhat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([image, sequence], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# get index with high probability\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brad/Desktop/EdLight%20Assessment/build_model.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m yhat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(yhat)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/engine/training.py:2378\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2377\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2378\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2380\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/engine/data_adapter.py:1305\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1303\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1305\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[1;32m   1306\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   1307\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:505\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    504\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    506\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:713\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    709\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    711\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 713\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[1;32m    715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:752\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    749\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    750\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m    751\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 752\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3407\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3408\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3409\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   3410\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3411\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "count = 0\n",
    "for key in test:\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    # get actual caption\n",
    "    captions = mappings[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, img_features[key], tokenizer, max_length) \n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "    \n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
